# -*- coding: utf-8 -*-
"""Final_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Pu58F7IODBOj6bA8pqHP0lWCC6wtd0S
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import requests
import csv
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import statsmodels.api as sm
from statsmodels.tsa import stattools
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.stattools import acf, pacf
import os
from pandas import Series
from pandas import concat
from pandas import read_csv
from pandas import datetime
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler
# import tensorflow as tf
# config = tf.ConfigProto()
# config.gpu_options.allow_growth = True
# config.gpu_options.per_process_gpu_memory_fraction = 0.4
# sess = tf.Session(config=config)

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping
from keras.layers import LSTM
from math import sqrt
from matplotlib import pyplot
from numpy import array
from sklearn.preprocessing import MinMaxScaler
os.environ['CUDA_VISIBLE_DEVICES']=''
# %matplotlib inline
plt.rcParams['figure.figsize'] = (15, 6)

from numpy.random import seed
seed(1)
import tensorflow as tf
tf.random.set_seed(2) 
#from tensorflow import set_random_seed
#set_random_seed(2)

def get_load_data(date):
    url = 'http://www.delhisldc.org/Loaddata.aspx?mode='
    print('Scraping ' + date, end=' ')
    resp = requests.get(url + date) # send a get request the url, get response
    soup = BeautifulSoup(resp.text, 'lxml') # Yummy HTML soup
    table = soup.find('table', {'id':'ContentPlaceHolder3_DGGridAv'}) # get the table from html
    trs = table.findAll('tr') # extract all rows of the table
    if len(trs[1:])==288: # no need to create csv file, if there's no data
        with open('monthdata.csv', 'a') as f:  #'a' makes sure the values are appended at the end of the already existing file
            writer = csv.writer(f)
            for tr in trs[1:]:
                time, delhi = tr.findChildren('font')[:2]
                writer.writerow([date + ' ' + time.text, delhi.text])
    if len(trs[1:]) != 288:
        print('Some of the load values are missing..')
    else:
        print('Done')

for i in range(31, 0, -1):
    yesterday = datetime.today() - timedelta(i)
    yesterday = yesterday.strftime('%d/%m/%Y')
    get_load_data(yesterday)

for i in range(31, 0, -1):
    yesterday = datetime.today() - timedelta(i)
    yesterday = yesterday.strftime('%d/%m/%Y')
    try:
        print(data[yesterday].shape[0])
    except:
        print(yesterday, 'not found')

!head monthdata.csv

data = pd.read_csv('monthdata.csv', header=None, names=['datetime', 'load'], index_col=[0], parse_dates=[0], infer_datetime_format=True)

data['23/02/2021'].plot()
data.plot()
plt.rcParams['figure.figsize'] = (20, 6)
plt.show()

data['10/02/2021':'18/02/2021']

data['18/02/2021']

set(data.index.date)

data[:'15/02/2021'].plot()
plt.rcParams['figure.figsize'] = (15, 6)
plt.show()

# data = data.asfreq(freq='H', method='bfill')

data.shape

# number of unique dates in the data
len(set(data.index.date))

data.tail()

plt.rcParams['figure.figsize'] = (15, 6)
decompfreq = 288 #daily freq
from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(data[:'23/02/2021'], freq=decompfreq, model='aditive')
result = seasonal_decompose(data, freq=decompfreq, model='aditive')

result.plot()
plt.show()

"""
### Making the data stationary
"""



"""## Detrending"""

plt.rcParams['figure.figsize'] = (15, 6)
decompfreq = 288 #daily freq
result = seasonal_decompose(data, freq=decompfreq, model='aditive')
result.plot()
plt.show()

dt_data = data.diff(1).dropna()  # detrended data

plt.rcParams['figure.figsize'] = (15, 6)
decompfreq = 288 #daily freq
result = seasonal_decompose(dt_data, freq=decompfreq, model='aditive')
result.plot()
plt.show()

"""### Removing seasonality"""

dt_data.plot()

#takes more time to run 
lag_pacf = pacf(dt_data['load'].values, nlags = 2000)
lag_acf = acf(dt_data['load'].values, nlags = 2000)

np.argsort(lag_pacf)  #returns the indices that would sort the array in ascending order

np.argsort(lag_acf)

plt.subplot(121) 
plt.plot(lag_pacf)
plt.axhline(y=0,linestyle='--')
plt.axhline(y=-1.96/np.sqrt(len(dt_data['load'])), linestyle='--')
plt.axhline(y=1.96/np.sqrt(len(dt_data['load'])), linestyle='--')

# critical value determination: https://stats.stackexchange.com/a/185553/181916

plt.subplot(121) 
plt.plot(lag_acf)
plt.axhline(y=0,linestyle='--')
plt.axhline(y=-1.96/np.sqrt(len(dt_data['load'])), linestyle='--')
plt.axhline(y=1.96/np.sqrt(len(dt_data['load'])), linestyle='--')

# critical value determination: https://stats.stackexchange.com/a/185553/181916

ds_dt_data = dt_data.diff(288).dropna()  # deseasonalized + detrended data

decompfreq = 288 #daily freq
result = seasonal_decompose(ds_dt_data, freq=decompfreq, model='aditive')
result.plot()
plt.show()

ds_dt_data.plot()

"""## Rescale the data"""

ds_dt_data.shape



scaler = MinMaxScaler(feature_range=(-1, 1))
scaler = scaler.fit(ds_dt_data['load'].values.reshape(-1, 1))

ds_dt_data['load1'] = ds_dt_data.shift(-1)['load']
ds_dt_data['load2'] = ds_dt_data.shift(-2)['load']
ds_dt_data['load3'] = ds_dt_data.shift(-3)['load']
ds_dt_data = ds_dt_data.dropna()

ds_dt_data = scaler.transform(ds_dt_data)  # don't use .values

split_idx = int(0.9 * len(ds_dt_data))
train, val = ds_dt_data[:split_idx], ds_dt_data[split_idx:]

train.shape, val.shape

X, y = train[:, 0], train[:, 1:]
X_val, y_val = val[:, 0], val[:, 1:]

y_val

plt.plot(range(len(X)), X)
plt.plot(range(len(X), len(X_val) + len(X)), X_val);

"""#### Note: to get more insight of data being used in LSTM/RNN/GRU training see `LSTM-eda.ipynb` file

### Load and prepare data in required format
"""

data = pd.read_csv('/content/monthdata.csv', header=None, names=['datetime', 'load'], index_col=[0], parse_dates=[0], infer_datetime_format=True)
df = pd.DataFrame(columns=['time'] + list(map(str, range(int(data.shape[0] / 288)))))
for idx, time in enumerate(sorted(set(data.index.time))):
    df.loc[idx] = [time.strftime(format='%H:%M:%S')] + list(data.at_time(time)['load'].values)
df.index = df['time']
df = df.drop('time', 1)
dt_df = df.diff(1, axis=1)
dt_df = dt_df.dropna(axis=1)
scaler = MinMaxScaler(feature_range=(-1, 1))
scaler = scaler.fit(dt_df.values.reshape(-1, 1))
dt_df = scaler.transform(dt_df)  # dt_df is now a numpy array
split_idx = int(len(dt_df) * 0.8)
train, val = dt_df[:split_idx, :], dt_df[split_idx:, :]

def prepare_data(data, nlags):
    '''prepares data for LSTM model, x=last nlags values, y=(nlags+1)'th value'''
    data_x, data_y = [], []
    for i in range(data.shape[0]):
        for j in range(0, data.shape[1]-nlags):
            data_x.append(data[i, j:j+nlags])
            data_y.append(data[i, j+nlags])
    data_x = np.array(data_x)
    data_y = np.array(data_y).reshape(-1, 1)
    return data_x, data_y

nlags = 10
train_x, train_y = prepare_data(train, nlags)
val_x, val_y = prepare_data(val, nlags)
train_x = train_x.reshape(train_x.shape[0], 1, nlags)
val_x = val_x.reshape(val_x.shape[0], 1, nlags)
df_last_nlags_plus_one = df.loc[:, df.columns[-nlags-1:]]  # slice last nlags+1 days from df, will be used in prediction of yesterday's data
dt_df_last_nlags = df_last_nlags_plus_one.diff(1, axis=1).dropna(axis=1)  #taking last 21 days, differencing and dropping the nan value
dt_df_last_nlags = scaler.transform(dt_df_last_nlags)  # df is now a numpy array
X = dt_df_last_nlags.reshape(dt_df_last_nlags.shape[0], 1, nlags)  # nlags=20
load = np.load('25nov.npy')

"""## Model training"""

model = Sequential()
model.add(LSTM(1, batch_input_shape=(1, train_x.shape[1], train_x.shape[2]), stateful=True))
#model.add(LSTM(1, stateful=True, return_sequences=True))
# model.add(LSTM(1, stateful=True))
# model.add(Dense(train_y.shape[1]))
model.add(Dense(train_y.shape[1]))
model.compile(loss='mean_squared_error', optimizer='sgd')

history = model.fit(train_x, train_y, epochs=30, batch_size=1, verbose=1, validation_data=(val_x, val_y), shuffle=False)

# fit network
early_stopping_counter = 0
last_loss = np.inf
for i in range(10):
    history = model.fit(train_x, train_y, epochs=1, batch_size=1, verbose=1, validation_data=(val_x, val_y), shuffle=False)
    model.reset_states()
    val_loss = history.history['val_loss'][0]
    if val_loss > last_loss:
        early_stopping_counter += 1
    else:    
        early_stopping_counter = 0
    last_loss = val_loss
    if early_stopping_counter == 3:
#         break
        pass

"""### NOTE: model needs to be trained for atleast 10 epochs to make decent predictions, use early stop callback afterwards"""

val_y_pred = model.predict(val_x, batch_size=1, verbose=1)
inverted_val_y = scaler.inverse_transform(val_y)
inverted_val_y_pred = scaler.inverse_transform(val_y_pred)
Y = model.predict(X, batch_size=1)  # predict for today's values
inv_Y = scaler.inverse_transform(Y)  # invert to detrended values' scale
rescaled_Y = [x+y for x, y in zip(inv_Y[:, 0], df.iloc[:, -1])]  # last day's values added to inv_Y to get it to original scale

"""the whole process of inverting to orignal scale is same for val_y and val_y_pred (value addition only), so values are going to be close if they are now

"""

val_RMSE = np.sqrt(np.sum(np.square(np.array(inverted_val_y_pred) - np.array(inverted_val_y))) / len(inverted_val_y)) 
RMSE = np.sqrt(np.sum(np.square(np.array(rescaled_Y) - load)) / len(load)) 
print(val_RMSE, RMSE)

plt.plot(load)
plt.plot(rescaled_Y)
plt.legend(['load', 'pred'])



"""#### References:

https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/
https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/
"""















# !jupyter nbconvert --to script LSTM-v2.ipynb







# ## get today's load

# def get_load_data(date):
#     load=[]
#     url = 'http://www.delhisldc.org/Loaddata.aspx?mode='
#     print('Scraping ' + date, end=' ')
#     resp = requests.get(url + date) # send a get request to the url, get response
#     soup = BeautifulSoup(resp.text, 'lxml') # Yummy HTML soup
#     table = soup.find('table', {'id':'ContentPlaceHolder3_DGGridAv'}) # get the table from html
#     trs = table.findAll('tr') # extract all rows of the table
#     if len(trs[1:])==288: # no need to create csv file, if there's no data
#         with open('monthdata.csv', 'a') as f:  #'a' makes sure the values are appended at the end of the already existing file
            
#             for tr in trs[1:]:
#                 time, delhi = tr.findChildren('font')[:2]
#                 load.append(delhi.text)
#     if len(trs[1:]) != 288:
#         print('Some of the load values are missing..')
#     else:
#         print('Done')
#     return load

# yesterday = datetime.today() - timedelta(1)
# yesterday = yesterday.strftime('%d/%m/%Y')
# load = get_load_data(yesterday)
# load = [float(x) for x in load]
# np.save('25nov.npy', load)